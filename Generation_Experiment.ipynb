{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpN7djOVwluU"
      },
      "source": [
        "# **Generation Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nBwytexAOMi",
        "outputId": "8fdbdf99-1fca-4abd-b174-bf8fd52ca113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.2/267.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --quiet --upgrade bitsandbytes langchain langchain-community langchain-huggingface transformers beautifulsoup4 faiss-gpu rank_bm25 lark qdrant-client langchain-chroma langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBUuQT4-i8rr"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "import torch\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import re\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from operator import itemgetter\n",
        "from langchain import hub\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain.retrievers import EnsembleRetriever # Supports Ensembling of results from multiple retrievers\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from google.colab import userdata\n",
        "from langchain import PromptTemplate\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "import time\n",
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Query Decomposition:**\n",
        "\n",
        "Decomposition is a query re-writing technique that focuses on decomposing a question into a set of subquestions.\n",
        "\n",
        "This is applicable and effective for our use case as users planning a holiday tend to string together many requests in a single query. By breaking down a large queries into sub-queries, the retriever can retrieve more relevant documents to each sub-query and therefore, support the LLM in answering the whole query better"
      ],
      "metadata": {
        "id": "Z7qoHA5xueTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioBgd3TcyV1D"
      },
      "outputs": [],
      "source": [
        "# Prompt Decomposition template used by the LLM to help break a question into sub questions\n",
        "\n",
        "template = \"\"\"You are an expert at converting user travel questions into sub-questions. \\\n",
        "You have access to several documents about the different travel destinations. \\\n",
        "\n",
        "Perform query decomposition. Given a user question, break it down into the most specific sub questions you can \\\n",
        "which will help you answer the original question. Each sub question should be about a single concept/fact/idea.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
        "\"\"\"\n",
        "\n",
        "template_2 = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries each on a new line):\"\"\"\n",
        "\n",
        "prompt_decomposition = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "llm = ChatGroq()"
      ],
      "metadata": {
        "id": "C37JDhLytG5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISATFGYEj3Op"
      },
      "source": [
        "**Test Queries for Decomposition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbdsYx8ekD3r"
      },
      "outputs": [],
      "source": [
        "test_query_1 = \"When is the best time to go Finland and what is there to do\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEYEfrh3sa4s"
      },
      "source": [
        "<u>Decompsition of a question into sub-questions</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oHWZOyqkHz9"
      },
      "outputs": [],
      "source": [
        "# Apply the decompsition template and break down the questions into sub questions using the prompt decompsition pipeline\n",
        "questions = generate_queries_decomposition.invoke({\"question\":test_query_1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n6kubTdHbnF",
        "outputId": "d62f1f50-b541-4fb4-f173-a823aee1348c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1) when is the best time to visit finland for tourism?',\n",
              " '2) what activities are available in finland during different seasons?',\n",
              " '3) how does weather affect tourist attractions in finland?']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fix Best Retriever from Retreiver Evaluation**"
      ],
      "metadata": {
        "id": "naTfkeFmuvqa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "79961d6e240942518afa759dadff838d",
            "d7fa39c3f38044c19ab556fb588e4d6e",
            "5720e276a15e4882827358fa938f947f",
            "078859a9359f44c6b9c5992f698effec",
            "5d36e5551fea4cb8a95d8269e0ee434d",
            "18c56a4c2cd543a9a67ba116d3b3c34a",
            "28f03487ca6643d4bce753743bba544e",
            "ee5b07410d7c4508a2eb72c4b8d5811b",
            "568aad886de54ae2b47b34cf9d5f7ef2",
            "c59e599804804cada5a0d62fbcfcb734",
            "775460dd686644a5ab9e4261f5911ff5",
            "b4df8e9a3a774a9ba5615211bebe3c76",
            "dcc67640f76848bbb334343d96b8eda9",
            "f01cabe09d9a4e2aa411f402763025c8",
            "09b99aa142de47cab54b25478e1d85e9",
            "29d69e32bd42417bad4bf0c4117c11f1",
            "987f0caf12074418b68fce431f16c87a",
            "5307506d7d3c4bbbb5a00169b482d6a6",
            "0bbb0012a74b4e48a1c7ba03d71f5586",
            "4c5b219837a148078804072a2ac1d04f",
            "60c7ed623d8b4ae1968e7bc8c0051135",
            "a50aef6a09084667bc6b4ec5e2ed9399",
            "c14197671d0544e68e658cb3d89c1fd3",
            "b457313472654d4899932b86a56e7c96",
            "0c1131d723aa40b5a7588b3262482ed9",
            "d7f96e3d000046dc81ea329b84cb1bbf",
            "9b4edcab7ae4455db969af59d7874778",
            "c6cc94e1c02343b8ae527d912172e273",
            "ee180776f64b440a9eaff3bbfc2fa1b9",
            "d21a26ef26894231bdebd832839faf3e",
            "b0211fe9274c42b298b9c7ff84b3ef31",
            "d2edaf1b658a47fb899b278ad53eb829",
            "845221b124b143f6bc8af489e5fdeadf",
            "3bdbaf88ed8a4c18a08393b7df5b959d",
            "6c8360fd44af4ab3b609dd99e1a18f41",
            "86199a3e1809444b82ffdbaca9d0df3c",
            "1f95d27230e94b3fbcc475dc88ceb077",
            "46d3507cc14e4bee955ef6877a51019b",
            "9a575e638bc34692a60a1e9c7637712c",
            "c2fc889af9084347815a7f02f4144b72",
            "f2ea4d84eaed4641a949a02c6805396b",
            "4785de0795de48fdb69a40cdd4fe524f",
            "fc7e986dc7c348e2b42ac6f260ed9f8d",
            "dec114641fe0472d8963358385a5b387",
            "3e3d3271f34247d18644bbcbfdcceeaa",
            "9b9b6153e1c54dc1a3ec825bed68b2b6",
            "8599f1d3498f45a58a7f6fc6187d59e0",
            "66ee5a260b13424d8e176dd450c91d78",
            "c91e3439db6e4b8fa9067f766c8d14a7",
            "b46837cea5d64ad4822187f1ddda0a62",
            "8b4087a0269440be91ee8f0adcf7eb22",
            "100502e813df4904a028909c89a1d877",
            "6bee378bf76548679fdb9749a4cb8447",
            "33f94b85b75247749b5ea2ae92646129",
            "ae9efe94270b459b80d2fa42678282c8",
            "370013f8a6a2489986d1b0dcc6919b62",
            "83b57f6c929245efbd7173b4b9e8da45",
            "dc40e074188b4f9c9c49b04b8b39c5ab",
            "11e2fcbea23f4439b8e902366cef3eb2",
            "746df4c9b9874a35979b2b120d51190b",
            "867c76c46ada4ed39ae202fe34a16755",
            "2fd4b030412a482eb4545d390aece607",
            "ab661b6f5a8d4294b139f113ab0c3e5e",
            "c955a897de8a4b7f972d985860ee8033",
            "0a8e1b0340e544f084f9dbab56a5931a",
            "8b90075f6b044f209eeab86a675ef65b",
            "196b77b94f2044d4a13f0b31682e19f6",
            "bb028892f3b1428fb424877e2b503daf",
            "0255d994e61e4f269caf3c6af49e4bbd",
            "06b4745a9ab7435b95c34be197e24549",
            "71e558489ff740cf81c592a707287f46",
            "0c8a32b079bc4420ac995483b99847eb",
            "f678ae29983b42908c37ab1be0e37d08",
            "d64ab97030b1472e8ce5c5e83cc72e44",
            "0978e54400d64fef974fa37e19bd926b",
            "92509126d9124b8bb610383240cdd7c5",
            "c96e986e0baf4fda9bf9df02a9d71849",
            "8bf6af88f00b4885b025a6bb73642b98",
            "449ef416b1a64e42aa727c9ab78a26f1",
            "54abea0842bc4274b48902dd12cd049d",
            "f66fb6f93e4648109ea7ed55a078fe02",
            "b01ace2ec8804afd9bedeed403dc8bb8",
            "32d579f858e040c4a620aa3d08b26462",
            "0c42b552befd4a3f8a23776907fbc568",
            "c7c1e4cd53454495bd26c46172d3be3a",
            "b70b505a713e4eceb5831e8e2ed9b541",
            "a313126e9ccb4c1192dbd444b005859a",
            "897d485c228f44c692a5595e2ea7709a",
            "903e57d20841430e9d5beb2b323f8faf",
            "2ead5d3e2e614caab87483456e57e2d3",
            "869e370b4ba94e2a8f78b5b47a2b2a99",
            "1105d098f2cb4fb383805d53b9d44b8b",
            "b8f07acccb3c4572af452cdafbf85478",
            "4433fa727ccc4dee9ff4bf4fe411f18a",
            "fb3c5fef99784fbfa95642c20070aa3f",
            "4680b6add9124836bbaca5f9d3ff71a0",
            "bf2ecc3d042546668522db81f163e1b5",
            "2a3d40380c4647df9482253acabe71cb",
            "b743d320b20c409eaa1623f5c7b2b993",
            "aa0d32305f2a4fffaea1d8c8166c4f40",
            "004afbe281534659a7136e89e13aa3c1",
            "266c8c6011194616979a397bf9b80e51",
            "e9e85d431b7d40fe9700e6fd7806546f",
            "dac5c4f6d6df48bdab59d06cdd5c30d4",
            "73eadef2958a407293fe51751e294a2e",
            "52ba7e86209b4d6eaadbbc41e61142f9",
            "b5a05b8f14564b1484241897a20aa699",
            "2b58517d1e254adba1dc5ff726e643a1",
            "3954f81a4e2f4b74b5bf17f2e1d2f012",
            "d822c3f166314ec39dfeaa2fcc1e8b4f",
            "33380222b4b64da08a20420505b34dd1",
            "1834f8da6d214924a3c85f0ae9a6fd50",
            "0e3acbffca1f4d1194a6d8f846ce59ed",
            "562afefbf8d1466b8e22114a11e53391",
            "0b964ef6c6cb44d3924c550b85b73f88",
            "716661d3a72a46aa86f66e3ebd5e34eb",
            "8692bff937da495aab26f3de1d88ab31",
            "d7a3ec861fe74b0186252edd69291a99",
            "c6810835994a4e8c806562cde08bbb3c",
            "d70a82bdea3b40e7aa46a8ab2edd3663",
            "43016a8a70b04dadbed19c498f24b1fa"
          ]
        },
        "id": "GqsJjnHmEhj7",
        "outputId": "92a84f97-2c68-4ddc-c0f0-5acea853dccc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79961d6e240942518afa759dadff838d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4df8e9a3a774a9ba5615211bebe3c76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c14197671d0544e68e658cb3d89c1fd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bdbaf88ed8a4c18a08393b7df5b959d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e3d3271f34247d18644bbcbfdcceeaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "370013f8a6a2489986d1b0dcc6919b62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "196b77b94f2044d4a13f0b31682e19f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bf6af88f00b4885b025a6bb73642b98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "903e57d20841430e9d5beb2b323f8faf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa0d32305f2a4fffaea1d8c8166c4f40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33380222b4b64da08a20420505b34dd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialise the FAISS retriever\n",
        "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "index = faiss.IndexFlatL2(len(embeddings_model.embed_query(\"hello world\")))\n",
        "faiss_vector_store = FAISS(\n",
        "    embedding_function=embeddings_model,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")\n",
        "faiss_vector_store.add_documents(docs)\n",
        "retriever = faiss_vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) # num docs to return from FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw4mPOQfAqkS"
      },
      "source": [
        "<u>Build the Final Answer Recursively</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzVY9_CrAhpk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Prompt template to recursively answer sub questions and build up the answers\n",
        "Might need to modify prompt to ask it to use only the context\n",
        "\n",
        "question: sub-question to be answered\n",
        "q_a_pairs: Built up question-answer pairs that might be relevant\n",
        "context: context retrieved for the current sub-question\n",
        "Idea is to recursively answer each sub-question, using the current context and building upon previous answers to provide more comprehensive responses.\n",
        "'''\n",
        "template = \"\"\"Here is the question you need to answer:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
        "\"\"\"\n",
        "decomposition_prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRqtMD7QA7Ve"
      },
      "outputs": [],
      "source": [
        "# Utility function to format a given question and answer\n",
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "# Initialise the q_a_pairs to be empty at first\n",
        "q_a_pairs = \"\"\n",
        "\n",
        "# For each sub-question that we decomposed from our main question earlier\n",
        "for q in questions:\n",
        "  rag_chain = (\n",
        "  # Given {\"question\":q,\"q_a_pairs\":q_a_pairs}\n",
        "  {\"context\": itemgetter(\"question\") | retriever,  # Get the context relevant to the subquestion using the retriever\n",
        "    \"question\": itemgetter(\"question\"), # Get the subquestion\n",
        "    \"q_a_pairs\": itemgetter(\"q_a_pairs\")} # Get the built up qna pairs\n",
        "  | decomposition_prompt # Pass the arguments into the template\n",
        "  | llm.bind(skip_prompt=True)\n",
        "  | StrOutputParser()) # Get the result from the LLM\n",
        "\n",
        "  # Pass our rag chain the sub question and any prev built up q_a_pairs\n",
        "  answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "  q_a_pair = format_qa_pair(q,answer) # Format it as sub_question, answer\n",
        "  q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair # Update/Build the q_a_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "qJVYQ6f-KOjt",
        "outputId": "d55ef7a3-f787-4276-817d-d01e93d18cc6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"To address the question of how weather affects tourist attractions in Finland, we will consider the provided context and background information.\\n\\nFirstly, let's examine the details about accommodations in Finland:\\n\\n1. **Snow Activities**: The Glass huts in Skyfire village in Rovaniemi, Lapland are highlighted as one of the best places to stay. These glass huts provide a unique experience where tourists can observe the Northern Lights and enjoy the snowy landscape. This indicates that the presence of snow and the occurrence of the Northern Lights significantly influence the appeal and accessibility of this attraction.\\n\\n2. **Northern Lights Viewing**: The Northern Lights, also known as Aurora Borealis, are mentioned as a captivating phenomenon that draws tourists to Finland. Snowfall conditions are crucial for visibility because they create a clear sky conducive to observing the aurora. Without sufficient snow cover, the atmosphere might be too cloudy or polluted, reducing the chances of seeing the Northern Lights.\\n\\nAdditionally, the context mentions other seasonal activities influenced by weather:\\n\\n3. **Spring Activities**: Wildflower trails and bird watching are more feasible during the spring season when flowers start blooming and birds begin their migration back to Finland. However, the timing and extent of these activities depend on the weather conditions, particularly rainfall and temperature changes.\\n\\n4. **Summer Activities**: Water sports and beach activities are typically associated with warmer weather. While there are some coastal areas that offer swimming pools or outdoor pools, the overall emphasis remains on outdoor recreational activities suitable for warm climates.\\n\\n5. **Autumn Activities**: Hiking and nature walks are possible during autumn when the leaves change colors. However, the quality and duration of these activities could vary depending on the amount of precipitation and cooler temperatures.\\n\\nIn summary, the weather plays a significant role in shaping tourist attractions in Finland. Snow and the Northern Lights are highly dependent on weather conditions, while other seasonal activities benefit from favorable weather patterns. Clear skies and mild temperatures enhance the enjoyment of outdoor activities, whereas inclement weather can limit certain experiences\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYTT7jaWBbFG"
      },
      "source": [
        "<u>Build the Final Answer Individually</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avOg004jBdp9",
        "outputId": "a87043a9-7a56-4f87-8e00-08f9f4217244"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "<ipython-input-38-9197ef3514dd>:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
          ]
        }
      ],
      "source": [
        "# Answer each sub-question individually\n",
        "# RAG prompt\n",
        "'''\n",
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
        "    \"\"\"Perform RAG on each sub-question\"\"\"\n",
        "    # Generate the sub questions using the chain\n",
        "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
        "    # Initialize a list to hold RAG results of each sub-question\n",
        "    rag_results = []\n",
        "    for sub_question in sub_questions:\n",
        "        # Retrieve documents for each sub-question\n",
        "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
        "        # Use retrieved documents and sub-question to answer the sub question\n",
        "        answer = (prompt_rag | llm.bind(skip_prompt=True) | StrOutputParser()).invoke({\"context\": retrieved_docs,\n",
        "                                                                \"question\": sub_question})\n",
        "        # Append the answer to the sub question\n",
        "        rag_results.append(answer)\n",
        "    # Return the list of sub questions and their answers\n",
        "    return rag_results,sub_questions\n",
        "\n",
        "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "8Olh0GE3CZSa",
        "outputId": "10b707fa-e61d-4a94-856b-24887786b5a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"To answer your question, the best time to visit Finland for tourism is during winter months like December through February when the Northern Lights (auroras) are most visible due to long nights and cold temperatures. This period provides opportunities for activities like snowshoeing, skiing, and aurora watching. However, summer from June to August also offers unique experiences such as midnight sun and vibrant nature with activities including hiking, kayaking, and berry-picking. Additionally, there's a chance to experience the country's unique glass huts in Lapland during the winter season. Overall, both winter and summer offer distinct and memorable experiences in Finland. To get the most out of your trip, it's recommended to check local forecasts and plan accordingly based on your interests and preferences. Here’s a summary of the key points:\\n\\n- **Best Time to Visit:** Winter (December through February) for Northern Lights and snowy landscapes.\\n- **Summer Activities:** Hiking, kayaking, berry-picking, and exploring glass huts in Lapland.\\n- **General Tips:** Check weather forecasts and tailor your itinerary to maximize enjoyment of each season's unique offerings.\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Utility function to format a given question and answer\n",
        "def format_qa_pairs(questions, answers):\n",
        "    \"\"\"Format Q and A pairs\"\"\"\n",
        "    formatted_string = \"\"\n",
        "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
        "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "# Format the list of sub questions and their answers from just now\n",
        "context = format_qa_pairs(questions, answers)\n",
        "\n",
        "# Prompt template to use each individual sub-question and answer, as well as the main question\n",
        "template = \"\"\"Here is a set of Q+A pairs:\n",
        "\n",
        "{context}\n",
        "\n",
        "Use these to synthesize an answer to the question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm.bind(skip_prompt=True)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"context\":context,\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPQI0arPvLuw"
      },
      "source": [
        "**Conclusion**: To be filled in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goL7fLxZFicO"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}