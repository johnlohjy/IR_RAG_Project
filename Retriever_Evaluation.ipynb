{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retriever Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-requisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each experiment that you want to evaluate, you have ran the experiment code to get an experiment output\n",
    "\n",
    "2. For each experiment that you want to evaluate, you have added the experiment output folder to ```experiment_outputs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import evaluation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate Experiment 1: Hybrid/Ensemble Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binary Relevance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "experiment_1_binary_relevance_folder = os.path.join(os.getcwd(),'experiment_outputs','experiment_1_output','binary_relevance')\n",
    "f_name = 'hybrid_retriever_bm25_0.0_faiss_1.0_binary_relevance.csv'\n",
    "f_name_split = f_name.split('_')\n",
    "bm25_val = f_name_split[3]\n",
    "faiss_val = f_name_split[5]\n",
    "fp = os.path.join(experiment_1_binary_relevance_folder, f_name)\n",
    "full_df = pd.read_csv(fp)\n",
    "df_split = [group for query, group in full_df.groupby('query')]\n",
    "relevance_scores = []\n",
    "for df in df_split:\n",
    "    # Assign random 1s or 0s to the relevant col for testing purposes\n",
    "    df['relevant'] = [random.choice([0, 1]) for _ in range(len(df))]\n",
    "    relevance_scores.append(list(df['relevant']))\n",
    "evaluation.mean_average_precision(relevance_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_binary_relevance_folder = os.path.join(os.getcwd(),'experiment_outputs','experiment_1_output','binary_relevance')\n",
    "\n",
    "# Initialise dictionaries: { 'bm25_bm25weight_faiss_faiss_weight' = mean average precision}, { 'bm25_bm25weight_faiss_faiss_weight' = mean reciprocal rank}\n",
    "experiment_1_mean_ave_precision_res = {}\n",
    "experiment_1_mean_reciprocal_rank_res = {}\n",
    "# For each bm25 weight and faiss weight combination,\n",
    "# Calculate the mean average precision and mean reciprocal rank over all the queries\n",
    "for f_name in os.listdir(experiment_1_binary_relevance_folder):\n",
    "    f_name_split = f_name.split('_')\n",
    "    bm25_val = f_name_split[3]\n",
    "    faiss_val = f_name_split[5]\n",
    "    fp = os.path.join(experiment_1_binary_relevance_folder, f_name)\n",
    "    full_df = pd.read_csv(fp)\n",
    "    df_split = [group for query, group in full_df.groupby('query')]\n",
    "    relevance_scores = []\n",
    "    for df in df_split:\n",
    "        # TOCHANGE: Assign random 1s or 0s to the relevant col for testing purposes\n",
    "        df['relevant'] = [random.choice([0, 1]) for _ in range(len(df))]\n",
    "        relevance_scores.append(list(df['relevant']))\n",
    "    experiment_1_mean_ave_precision_res[f'bm25_{bm25_val}_faiss_{faiss_val}'] = evaluation.mean_average_precision(relevance_scores)\n",
    "    experiment_1_mean_reciprocal_rank_res[f'bm25_{bm25_val}_faiss_{faiss_val}'] = evaluation.mean_reciprocal_rank(relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best weightage for the mean average precision using binary relevance is:\n",
      "Weightage with bm25: 0.4, faiss: 0.6 with a value of 0.8092687074829932\n"
     ]
    }
   ],
   "source": [
    "max_value_map = max(experiment_1_mean_ave_precision_res.values())\n",
    "best_maps = {key: value for key, value in experiment_1_mean_ave_precision_res.items() if value == max_value_map}\n",
    "print('The best weightage for the mean average precision using binary relevance is:')\n",
    "for k,v in best_maps.items():\n",
    "    print(f'Weightage with bm25: {k.split('_')[1]}, faiss: {k.split('_')[-1]} with a value of {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best weightage for the mean reciprocal rank using binary relevance is:\n",
      "Weightage with bm25: 0.4, faiss: 0.6 with a value of 1.0\n",
      "Weightage with bm25: 0.8, faiss: 0.2 with a value of 1.0\n",
      "Weightage with bm25: 0.9, faiss: 0.1 with a value of 1.0\n"
     ]
    }
   ],
   "source": [
    "max_value_mrr = max(experiment_1_mean_reciprocal_rank_res.values())\n",
    "best_mrrs = {key: value for key, value in experiment_1_mean_reciprocal_rank_res.items() if value == max_value_mrr}\n",
    "print('The best weightage for the mean reciprocal rank using binary relevance is:')\n",
    "for k,v in best_mrrs.items():\n",
    "    print(f'Weightage with bm25: {k.split('_')[1]}, faiss: {k.split('_')[-1]} with a value of {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Score Relevance**\n",
    "\n",
    "Put at k at 5 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "experiment_1_score_relevance_folder = os.path.join(os.getcwd(),'experiment_outputs','experiment_1_output','score_relevance')\n",
    "# Initialise dictionaries: { 'bm25_bm25weight_faiss_faiss_weight' = mean normalised discounted cumulative gain}\n",
    "experiment_1_mean_normalised_discounted_cumulative_gain_res = {}\n",
    "# For each bm25 weight and faiss weight combination,\n",
    "# Calculate the normalised discounted cumulative gain over all the queries\n",
    "for f_name in os.listdir(experiment_1_score_relevance_folder):\n",
    "    f_name_split = f_name.split('_')\n",
    "    bm25_val = f_name_split[3]\n",
    "    faiss_val = f_name_split[5]\n",
    "    fp = os.path.join(experiment_1_score_relevance_folder, f_name)\n",
    "    full_df = pd.read_csv(fp)\n",
    "    df_split = [group for query, group in full_df.groupby('query')]\n",
    "    relevance_scores = []\n",
    "    for df in df_split:\n",
    "        # TOCHANGE: Assign random score between 0 and 5 to the relevant col for testing purposes\n",
    "        df['relevant'] = [random.choice([0, 1, 2, 3, 4, 5]) for _ in range(len(df))]\n",
    "        relevance_scores.append(evaluation.ndcg_at_k(list(df['relevant']),k))\n",
    "    experiment_1_mean_normalised_discounted_cumulative_gain_res[f'bm25_{bm25_val}_faiss_{faiss_val}'] = np.mean(relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best weightage for the mean normalised discounted cumulative gain using score relevance is:\n",
      "Weightage with bm25: 0.7, faiss: 0.3 with a value of 0.7836464358137009\n"
     ]
    }
   ],
   "source": [
    "max_value_map = max(experiment_1_mean_normalised_discounted_cumulative_gain_res.values())\n",
    "best_maps = {key: value for key, value in experiment_1_mean_normalised_discounted_cumulative_gain_res.items() if value == max_value_map}\n",
    "print('The best weightage for the mean normalised discounted cumulative gain using score relevance is:')\n",
    "for k,v in best_maps.items():\n",
    "    print(f'Weightage with bm25: {k.split('_')[1]}, faiss: {k.split('_')[-1]} with a value of {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
