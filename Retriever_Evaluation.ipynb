{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retriever Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-requisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each experiment that you want to evaluate, you have ran the experiment code to get an experiment output\n",
    "\n",
    "2. For each experiment that you want to evaluate, you have added the experiment output folder to ```experiment_outputs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import retriever_evaluation\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import ragas\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import LLMContextRecall\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **User Action Required**\n",
    "\n",
    "1. Run the code below to create the ```experiment_outputs``` folder\n",
    "\n",
    "2. Choose to add/upload the following folders\n",
    "- ```experiment_1_output```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = os.path.join(os.getcwd(), 'experiment_outputs')\n",
    "os.makedirs(experiment_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_files = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate Experiment 1: Hybrid/Ensemble Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binary Relevance: Mean Average Precision, Mean Reciprocal Rank**\n",
    "\n",
    "Mean Average Precision: \n",
    "\n",
    "Mean Reciprocal Rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_binary_relevance_folder = os.path.join(experiment_folder,'experiment_1_output','binary_relevance')\n",
    "\n",
    "# Initialise dictionaries: { 'bm25_bm25weight_faiss_faiss_weight' = mean average precision}, { 'bm25_bm25weight_faiss_faiss_weight' = mean reciprocal rank}\n",
    "experiment_1_mean_ave_precision_res = {}\n",
    "experiment_1_mean_reciprocal_rank_res = {}\n",
    "# For each bm25 weight and faiss weight combination,\n",
    "# Calculate the mean average precision and mean reciprocal rank over all the queries\n",
    "for f_name in os.listdir(experiment_1_binary_relevance_folder):\n",
    "    f_name_split = f_name.split('_')\n",
    "    bm25_val = f_name_split[3]\n",
    "    faiss_val = f_name_split[5]\n",
    "    fp = os.path.join(experiment_1_binary_relevance_folder, f_name)\n",
    "    full_df = pd.read_csv(fp)\n",
    "    df_split = [group for query, group in full_df.groupby('query')]\n",
    "    relevance_scores = []\n",
    "    for df in df_split:\n",
    "        # TOCHANGE: Assign random 1s or 0s to the relevant col for testing purposes\n",
    "        df['relevant'] = [random.choice([0, 1]) for _ in range(len(df))]\n",
    "        relevance_scores.append(list(df['relevant']))\n",
    "    experiment_1_mean_ave_precision_res[f'bm25_{bm25_val}_faiss_{faiss_val}'] = retriever_evaluation.mean_average_precision(relevance_scores)\n",
    "    experiment_1_mean_reciprocal_rank_res[f'bm25_{bm25_val}_faiss_{faiss_val}'] = retriever_evaluation.mean_reciprocal_rank(relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_map = max(experiment_1_mean_ave_precision_res.values())\n",
    "best_maps = {key: value for key, value in experiment_1_mean_ave_precision_res.items() if value == max_value_map}\n",
    "print('The best weightage for the mean average precision using binary relevance is:')\n",
    "for k,v in best_maps.items():\n",
    "    print(f'Weightage with bm25: {k.split('_')[1]}, faiss: {k.split('_')[-1]} with a value of {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_mrr = max(experiment_1_mean_reciprocal_rank_res.values())\n",
    "best_mrrs = {key: value for key, value in experiment_1_mean_reciprocal_rank_res.items() if value == max_value_mrr}\n",
    "print('The best weightage for the mean reciprocal rank using binary relevance is:')\n",
    "for k,v in best_mrrs.items():\n",
    "    print(f'Weightage with bm25: {k.split('_')[1]}, faiss: {k.split('_')[-1]} with a value of {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Score Relevance: Mean Normalised Discounted Cumulative Gain**\n",
    "\n",
    "For score relevance, put at k=5 first\n",
    "\n",
    "Mean Normalised Discounted Cumulative Gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "experiment_1_score_relevance_folder = os.path.join(experiment_folder,'experiment_1_output','score_relevance')\n",
    "# Initialise dictionaries: { 'bm25_bm25weight_faiss_faiss_weight' = mean normalised discounted cumulative gain}\n",
    "experiment_1_mean_normalised_discounted_cumulative_gain_res = {}\n",
    "# For each bm25 weight and faiss weight combination,\n",
    "# Calculate the normalised discounted cumulative gain over all the queries\n",
    "for f_name in os.listdir(experiment_1_score_relevance_folder):\n",
    "    f_name_split = f_name.split('_')\n",
    "    bm25_val = f_name_split[3]\n",
    "    faiss_val = f_name_split[5]\n",
    "    fp = os.path.join(experiment_1_score_relevance_folder, f_name)\n",
    "    full_df = pd.read_csv(fp)\n",
    "    df_split = [group for query, group in full_df.groupby('query')]\n",
    "    relevance_scores = []\n",
    "    for df in df_split:\n",
    "        # TOCHANGE: Assign random score between 0 and 5 to the relevant col for testing purposes\n",
    "        df['relevant'] = [random.choice([0, 1, 2, 3, 4, 5]) for _ in range(len(df))]\n",
    "        relevance_scores.append(retriever_evaluation.ndcg_at_k(list(df['relevant']),k))\n",
    "    experiment_1_mean_normalised_discounted_cumulative_gain_res[f'bm25_{bm25_val}_faiss_{faiss_val}'] = np.mean(relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_map = max(experiment_1_mean_normalised_discounted_cumulative_gain_res.values())\n",
    "best_maps = {key: value for key, value in experiment_1_mean_normalised_discounted_cumulative_gain_res.items() if value == max_value_map}\n",
    "print('The best weightage for the mean normalised discounted cumulative gain using score relevance is:')\n",
    "for k,v in best_maps.items():\n",
    "    print(f'Weightage with bm25: {k.split('_')[1]}, faiss: {k.split('_')[-1]} with a value of {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Context Recall with RAGAS**\n",
    "\n",
    "Calculate using\n",
    "- Reference/GT answer\n",
    "- Retrieved context results\n",
    "\n",
    "To estimate context recall from the Reference/GT answer, the Reference/GT answer is broken into claims\n",
    "\n",
    "Each claim in the Reference/GT answer is analysed by an LLM to determine if it can be attributed to the retrieved context or not \n",
    "\n",
    "```\n",
    "context_recall = number of reference claims that can be attributed to the retrieved context / number of reference claims\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Retrieval Experiment 1 and 2, save questions to a list\n",
    "questions = [\"best food eat Finland\", \"best food eat Iceland\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill in ground truth answers for each question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finland_gpt_answer = \"\"\"\n",
    "Finland's culinary traditions offer a rich array of flavors, reflecting its natural resources and cultural heritage. Here are some quintessential Finnish dishes to experience:\n",
    "\n",
    "Karjalanpiirakka (Karelian Pie)\n",
    "Originating from the Karelia region, these rye crust pastries are traditionally filled with rice porridge and often topped with egg butter. They are a beloved Finnish snack, commonly enjoyed across the country.\n",
    "\n",
    "Ruisleipä (Rye Bread)\n",
    "A staple in Finnish cuisine, this dense and dark bread is made from sourdough rye. It's typically enjoyed with butter, cheese, or cold cuts, and forms an essential part of daily meals.\n",
    "\n",
    "Kalakukko\n",
    "Hailing from the Savonia region, this traditional dish consists of fish (commonly perch or salmon) and pork baked inside a thick rye bread crust, creating a hearty and portable meal.\n",
    "\n",
    "Poronkäristys (Sautéed Reindeer)\n",
    "A specialty from Lapland, this dish features thinly sliced reindeer meat sautéed with onions and butter, typically served with mashed potatoes and lingonberry jam.\n",
    "\n",
    "Leipäjuusto (Bread Cheese)\n",
    "Also known as 'squeaky cheese' due to its texture, this mild cheese is often warmed and served with cloudberry jam, offering a unique combination of flavors.\n",
    "\n",
    "Lohikeitto (Salmon Soup)\n",
    "A creamy soup made with fresh salmon, potatoes, leeks, and dill, providing a comforting and flavorful experience, especially during colder months.\n",
    "\n",
    "Mustikkapiirakka (Blueberry Pie)\n",
    "This traditional dessert features wild Finnish blueberries baked into a pie, often enjoyed with vanilla sauce or ice cream.\n",
    "\n",
    "Exploring these dishes will provide a genuine taste of Finland's rich culinary heritage.\n",
    "\"\"\"\n",
    "\n",
    "iceland_gpt_answer = \"\"\"\n",
    "Iceland's culinary scene offers a rich tapestry of traditional dishes that reflect its unique heritage and natural resources. Here are some quintessential Icelandic foods to experience:\n",
    "\n",
    "Pylsur (Icelandic Hot Dog)\n",
    "A blend of lamb, pork, and beef, served in a soft bun with toppings like ketchup, sweet mustard, remoulade, and both raw and crispy fried onions. A popular spot to try this is Bæjarins Beztu Pylsur in Reykjavík, renowned for its delicious hot dogs.\n",
    "\n",
    "Plokkfiskur (Fish Stew)\n",
    "A hearty mix of white fish (such as cod or haddock), potatoes, onions, and béchamel sauce. This comforting dish showcases Iceland's rich fishing traditions.\n",
    "\n",
    "Hangikjöt (Smoked Lamb)\n",
    "Traditionally smoked over birch or dried sheep dung, this lamb is typically served thinly sliced with flatbread or potatoes, especially during festive seasons.\n",
    "\n",
    "Kjötsúpa (Lamb Soup)\n",
    "A nourishing soup made with lamb, root vegetables, and herbs, offering warmth during Iceland's colder months.\n",
    "\n",
    "Skyr\n",
    "A thick, creamy dairy product similar to yogurt but technically a cheese. It's enjoyed plain or with added flavors like berries and is a staple in Icelandic diets.\n",
    "\n",
    "Harðfiskur (Dried Fish)\n",
    "Wind-dried fish, often cod or haddock, served with salted butter. This protein-rich snack has been a traditional staple for centuries.\n",
    "\n",
    "Kleinur\n",
    "A twisted doughnut-like pastry, deep-fried and mildly sweet, commonly enjoyed with coffee.\n",
    "\n",
    "For a contemporary twist on traditional Icelandic cuisine, consider dining at Dill in Reykjavík. As the first Icelandic restaurant awarded a Michelin star, Dill offers innovative dishes that highlight local ingredients.\n",
    "\n",
    "Exploring these dishes will provide a genuine taste of Iceland's culinary heritage.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "ground_truths = [[finland_gpt_answer],\n",
    "                [iceland_gpt_answer]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use RAGAS library to calculate context recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_binary_relevance_folder = os.path.join(os.getcwd(),'experiment_outputs','experiment_1_output','binary_relevance')\n",
    "f_name = 'hybrid_retriever_bm25_0.0_faiss_1.0_binary_relevance.csv'\n",
    "fp = os.path.join(experiment_1_binary_relevance_folder, f_name)\n",
    "full_df = pd.read_csv(fp)\n",
    "df_split = [group for query, group in full_df.groupby('query')]\n",
    "contexts = []\n",
    "for df in df_split:\n",
    "  contexts.append(list(df['retrieved_doc']))\n",
    "\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    retrieved_contexts=[\"Paris is the capital of France.\"], \n",
    ")\n",
    "\n",
    "context_recall = LLMContextRecall()\n",
    "await context_recall.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
