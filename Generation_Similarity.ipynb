{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generation Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade bitsandbytes langchain langchain-community langchain-huggingface transformers beautifulsoup4 faiss-gpu rank_bm25 lark qdrant-client langchain-chroma langchain_groq ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import re\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.storage import InMemoryStore\n",
    "from operator import itemgetter\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.retrievers import EnsembleRetriever # Supports Ensembling of results from multiple retrievers\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from google.colab import userdata\n",
    "from langchain import PromptTemplate\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from google.colab import files\n",
    "import time\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import ResponseRelevancy, LLMContextRecall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_files = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_response_docs_feed_5_parent_chunk_size_360.json', 'r') as file:\n",
    "    responses = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_results_docs_feed_5_parent_chunk_size_360.json', 'r') as file:\n",
    "    results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generation_eval.json', 'r') as file:\n",
    "    generation_eval = json.load(file)\n",
    "\n",
    "cross_encoder_embedings_model_name = \"BAAI/bge-reranker-large\"\n",
    "cross_encoder_model = HuggingFaceCrossEncoder(model_name=cross_encoder_embedings_model_name)\n",
    "\n",
    "def process_response_answer_only(response):\n",
    "  answer_only = response.split('Citations')[0].replace(\"\\n\", \" \").strip()\n",
    "  return answer_only\n",
    "\n",
    "question_numbers = ['question_1', 'question_2', 'question_3', 'question_4', 'question_5']\n",
    "similarity_gpt_and_ours = []\n",
    "for question_number in question_numbers:\n",
    "  parent_question = generation_eval[question_number]['question']\n",
    "  ground_truth_answer = generation_eval[question_number]['ground_truth']\n",
    "  sub_answers = []\n",
    "  for response in responses:\n",
    "    for k,v in response.items():\n",
    "      if response[k]['parent_question'] == parent_question:\n",
    "        sub_answers.append(process_response_answer_only(response[k]['answer']))\n",
    "  model_answer = '.'.join(sub_answers)\n",
    "\n",
    "  similarity_gpt_and_ours.append(cross_encoder_model.score((ground_truth_answer,model_answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['similarity_with_ground_truth'] = {\n",
    "    'all_scores': similarity_gpt_and_ours,\n",
    "    'mean': np.mean(similarity_gpt_and_ours)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_results_docs_feed_5_parent_chunk_size_360.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
